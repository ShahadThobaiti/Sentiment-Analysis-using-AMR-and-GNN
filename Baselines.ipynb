{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baselines.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPplq5o9B6pp9JmaXv5B2M1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShahadThobaiti/SR4NLP_project/blob/main/Baselines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install dependencies:"
      ],
      "metadata": {
        "id": "Uv5059JEZpHt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmkklnO8Zlx1",
        "outputId": "2edf1341-bdd8-4fe1-eee0-a191687a3c40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "pip install sklearn xgboost nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download other NLTK dependencies before the first run\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# import all libraries used below\n",
        "import csv, re, string\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm04ZIBcZ1W0",
        "outputId": "4d3686cd-4233-4f4f-df38-a68f54e9bb28"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatize tweets for baseline models."
      ],
      "metadata": {
        "id": "Iai-lPkfaGkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatizer(tweet):\n",
        "\t\"\"\"\n",
        "\t\tremove stopwords, uncommon words, unknownwords etc.\n",
        "\t\"\"\"\n",
        "\ttoken_words = pos_tag(word_tokenize(tweet))\n",
        "\tlemmatizer = WordNetLemmatizer()\n",
        "\troot_words = []\n",
        "\tfor word, tag in token_words:\n",
        "\t\tif not wordnet.synsets(word):\n",
        "\t\t\tcontinue\n",
        "\t\tif tag.startswith('NN'):\n",
        "\t\t\tword_lematizer = lemmatizer.lemmatize(word, pos='n')\n",
        "\t\telif tag.startswith('VB'):\n",
        "\t\t\tword_lematizer = lemmatizer.lemmatize(word, pos='v')\n",
        "\t\telif tag.startswith('JJ'):\n",
        "\t\t\tword_lematizer = lemmatizer.lemmatize(word, pos='a')\n",
        "\t\telif tag.startswith('R'):\n",
        "\t\t\tword_lematizer = lemmatizer.lemmatize(word, pos='r')\n",
        "\t\telse:\n",
        "\t\t\tword_lematizer = lemmatizer.lemmatize(word)\n",
        "\n",
        "\t\troot_words.append(word_lematizer)\n",
        "\n",
        "\tfiltered_words = [word.lower() for word in root_words if word not in set(stopwords.words('english'))]\n",
        "\treturn \" \".join(filtered_words)\n",
        "\n",
        "def preprocess_text(tweet):\n",
        "\t\"\"\"\n",
        "\t\tremove punctuations, emojis, tabs, spaces, new lines, etc.\n",
        "\t\"\"\"\n",
        "\ttext = re.sub('@[^\\s]+|[hH]ttps\\S\\S\\St.co(.)*|[0-9]|[^\\w\\s]','',tweet)\n",
        "\t# Regular expression to handle emojis, emoticons, and extraneous symbols \n",
        "\ttext = text.replace(\"\\n\",\" \")\n",
        "\ttext = text.translate(str.maketrans({a:None for a in string.punctuation}))\n",
        "\ttext = text.lower()\n",
        "\treturn lemmatizer(tweet)"
      ],
      "metadata": {
        "id": "vF3a6iZLaNNX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load tweets and lemmatize them."
      ],
      "metadata": {
        "id": "ibF8x8W9ab7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "with open('train.csv', 'r') as csv_file, open('test.csv', 'r') as csv_file2:\n",
        "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "        csv_reader2 = csv.reader(csv_file2, delimiter=',')\n",
        "        next(csv_reader, None)\n",
        "        next(csv_reader2, None)\n",
        "        x_train, y_train, x_test, y_test = [], [], [], []\n",
        "        for row in csv_reader:\n",
        "            x_train.append(preprocess_text(row[2]))\n",
        "            if row[1] == 'Positive':\n",
        "                y_train.append(2)\n",
        "            elif row[1] == 'Negative':\n",
        "                y_train.append(0)\n",
        "            else:\n",
        "                y_train.append(1)\n",
        "\n",
        "        for row in csv_reader2:\n",
        "            x_test.append(preprocess_text(row[2]))\n",
        "            if row[1] == 'Positive':\n",
        "                y_test.append(2)\n",
        "            elif row[1] == 'Negative':\n",
        "                y_test.append(0)\n",
        "            else:\n",
        "                y_test.append(1)\n",
        "\n",
        "        print(x_train[1],y_train[1])\n",
        "        print(len(x_train),len(y_train),len(x_test),len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuvnLCfOanrH",
        "outputId": "405a2e06-c16a-4999-c5da-d4225b7618b3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saudi Arabia acquired a 8.2% stake in Carnival (approximately 43.5 million shares). The shares were at very low prices and since the beginning of the year the stock has lost 81% of the value since the #cruise industry is struggling with the #coronav 0\n",
            "1600 1600 400 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize lemmatized tweets."
      ],
      "metadata": {
        "id": "TzaIyR_navYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vect = CountVectorizer()\n",
        "transformer = TfidfTransformer(norm='l2',sublinear_tf=True)\n",
        "x_train_counts = count_vect.fit_transform(x_train)\n",
        "x_train_tfidf = transformer.fit_transform(x_train_counts)\n",
        "x_test_counts = count_vect.transform(x_test)\n",
        "x_test_tfidf = transformer.transform(x_test_counts)"
      ],
      "metadata": {
        "id": "TF8doLA7aqLp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Baseline classifiers and regressors."
      ],
      "metadata": {
        "id": "GDU0q1bbbI63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluations = [\"Baseline Models\", \"Accuracy\", \"F1-Score\"]\n",
        "\"\"\" baseline classifier - Decision Tree \"\"\"\n",
        "dtmodel = DecisionTreeClassifier(criterion='entropy', random_state=1)\n",
        "dtmodel.fit(x_train_tfidf,y_train)\n",
        "predictions = dtmodel.predict(x_test_tfidf)\n",
        "f1 = f1_score(y_test,predictions,average='micro')\n",
        "evaluations.append([\"Decision Tree\",accuracy_score(y_test,predictions)*100,f1])\n",
        "\n",
        "\"\"\" baseline classifier - Random Forest \"\"\"\n",
        "model = RandomForestClassifier(n_estimators=200)\n",
        "model.fit(x_train_tfidf,y_train)\n",
        "predictions = model.predict(x_test_tfidf)\n",
        "f1 = f1_score(y_test,predictions,average='micro')\n",
        "evaluations.append([\"Random Forest\",accuracy_score(y_test,predictions)*100,f1])\n",
        "\n",
        "\"\"\" baseline regressor - Logistic Regression \"\"\"\n",
        "logmodel = LogisticRegression(random_state=400)\n",
        "logmodel.fit(x_train_tfidf,y_train)\n",
        "predictions = logmodel.predict(x_test_tfidf)\n",
        "f1 = f1_score(y_test,predictions.round(),average='micro')\n",
        "evaluations.append([\"Logistic Regression\",accuracy_score(y_test,predictions.round())*100,f1])\n",
        "\n",
        "# \"\"\" baseline regressor - Gradient Boost Regressor \"\"\"\n",
        "gbmodel= GradientBoostingRegressor(n_estimators= 550, learning_rate= 0.1, max_depth= 3)\n",
        "gbmodel.fit(x_train_tfidf,y_train)\n",
        "predictions = gbmodel.predict(x_test_tfidf)\n",
        "f1 = f1_score(y_test,predictions.round(),average='micro')\n",
        "evaluations.append([\"Gradient Boost Regression\",accuracy_score(y_test,predictions.round())*100,f1])\n",
        "\n",
        "# \"\"\" baseline regressor - XGBoost \"\"\"\n",
        "xgbmodel=XGBClassifier(random_state=22,learning_rate=0.9)\n",
        "xgbmodel.fit(x_train_tfidf,y_train)\n",
        "predictions = xgbmodel.predict(x_test_tfidf)\n",
        "f1 = f1_score(y_test,predictions.round(),average='micro')\n",
        "evaluations.append([\"XGBoost Regression\",accuracy_score(y_test,predictions.round())*100,f1])\n",
        "\n",
        "print(evaluations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRMX4nOrbO2K",
        "outputId": "4fa4787a-4a78-4d96-e71b-30f1a3e1195a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Baseline Models', 'Accuracy', 'F1-Score', ['Decision Tree', 38.75, 0.3875], ['Random Forest', 51.24999999999999, 0.5125], ['Logistic Regression', 51.74999999999999, 0.5175], ['Gradient Boost Regression', 33.0, 0.33], ['XGBoost Regression', 48.5, 0.485]]\n"
          ]
        }
      ]
    }
  ]
}