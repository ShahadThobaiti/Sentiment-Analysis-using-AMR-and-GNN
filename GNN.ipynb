{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Iterable\n",
    "from pydot import Dot, graph_from_dot_data, Edge\n",
    "from graphviz.graphs import BaseGraph\n",
    "from graphviz import Source\n",
    "import amrlib\n",
    "from amrlib.graph_processing.amr_plot import AMRPlot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv, pickle\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract nodes and edges from AMR graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cite: https://stackoverflow.com/questions/47426249/finding-list-of-edges-in-graphviz-in-python \n",
    "def get_graph_dot_obj(graph_spec) -> List[Dot]:\n",
    "    \"\"\"Get a dot (graphs) object list from a variety \n",
    "    of possible sources (postelizing inputs here)\"\"\"\n",
    "    _original_graph_spec = graph_spec\n",
    "    if isinstance(graph_spec, (BaseGraph, Source)):\n",
    "        # get the source (str) from a graph object\n",
    "        graph_spec = graph_spec.source\n",
    "    if isinstance(graph_spec, str):\n",
    "        # get a dot-graph from dot string data\n",
    "        graph_spec = graph_from_dot_data(graph_spec)\n",
    "    # make sure we have a list of Dot objects now\n",
    "    assert isinstance(graph_spec, list) and all(\n",
    "        isinstance(x, Dot) for x in graph_spec\n",
    "    ), (\n",
    "        f\"Couldn't get a proper dot object list from: {_original_graph_spec}. \"\n",
    "        f\"At this point, we should have a list of Dot objects, but was: {graph_spec}\"\n",
    "    )\n",
    "    return graph_spec\n",
    "\n",
    "def get_edges(graph_spec, label = False):\n",
    "    \"\"\"Get a list of edges for a given graph (or list of lists thereof).\n",
    "    If ``postprocess_edges`` is ``None`` the function will return ``pydot.Edge`` objects from\n",
    "    which you can extract any information you want.\n",
    "    By default though, it is set to extract the node pairs for the edges, and you can\n",
    "    replace with any function that takes ``pydot.Edge`` as an input.\n",
    "    \"\"\"\n",
    "    graphs = get_graph_dot_obj(graph_spec)\n",
    "    n_graphs = len(graphs)\n",
    "    if n_graphs > 1:\n",
    "        return [get_edges(graph) for graph in graphs]\n",
    "    elif n_graphs == 0:\n",
    "        raise ValueError(f\"Your input had no graphs\")\n",
    "    else:\n",
    "        graph = graphs[0]\n",
    "        edges = graph.get_edges()\n",
    "        edges_list = []\n",
    "        if not label:\n",
    "            for edge in edges:\n",
    "                r1, r2 = graph.get_node(edge.get_source())[0].get_label().strip('\\\"').strip('\\\\').strip('\\\"'), graph.get_node(edge.get_destination())[0].get_label().strip('\\\"').strip('\\\\').strip('\\\"')\n",
    "                if '/' in r1:\n",
    "                    r1 = r1.split('/')[1]\n",
    "                elif '\\\\' in r1:\n",
    "                    r1 = r1.split('\\\\')[0]\n",
    "                \n",
    "                if '/' in r2:\n",
    "                    r2 = r2.split('/')[1]\n",
    "                elif '\\\\' in r1:\n",
    "                    r2 = r2.split('\\\\')[0]\n",
    "\n",
    "                edges_list.append([r1,r2])\n",
    "        else:\n",
    "            for edge in edges:\n",
    "                r1, r2, r3 = graph.get_node(edge.get_source())[0].get_label().strip('\\\"').strip('\\\\').strip('\\\"'), graph.get_node(edge.get_destination())[0].get_label().strip('\\\"').strip('\\\\').strip('\\\"'), edge.get_label().strip('\\\"')[1:]\n",
    "                if '/' in r1:\n",
    "                    r1 = r1.split('/')[1]\n",
    "                elif '\\\\' in r1:\n",
    "                    r1 = r1.split('\\\\')[0]\n",
    "                \n",
    "                if '/' in r2:\n",
    "                    r2 = r2.split('/')[1]\n",
    "                elif '\\\\' in r1:\n",
    "                    print(\"called\")\n",
    "                    r2 = r2.split('\\\\')[0]\n",
    "\n",
    "                edges_list.append([r1,r2,r3])\n",
    "        \n",
    "        return edges_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save large intermediate results (Only used for the first run). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_AMR.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    next(csv_reader, None)\n",
    "    g_train = []\n",
    "    for row in tqdm(csv_reader, total=1600):\n",
    "        AP = AMRPlot()\n",
    "        AP.build_from_graph(entry = row)\n",
    "        edges = get_edges(AP.graph, label=True)\n",
    "        g_train.append(edges)\n",
    "\n",
    "    np.save('g_train',g_train)\n",
    "    print(g_train[1])\n",
    "\n",
    "len(g_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66c37180a0b49189f800f01e1fef0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring epigraph data for duplicate triple: ('b', ':mod', 'p3')\n",
      "ignoring epigraph data for duplicate triple: ('c', ':ARG0', 'h')\n",
      "ignoring epigraph data for duplicate triple: ('w2', ':ARG0', 'p')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['create-01', 'company', 'ARG0'], ['company', 'name', 'name'], ['create-01', 'job', 'ARG1'], ['job', 'new-01', 'ARG1-of'], ['job', 'multiple', 'quant'], ['create-01', 'footprint', 'location'], ['footprint', 'company', 'poss'], ['footprint', 'country', 'location'], ['country', 'name', 'name'], ['create-01', 'increase-01', 'purpose'], ['increase-01', 'company', 'ARG0'], ['increase-01', 'effort-01', 'ARG1'], ['effort-01', 'company', 'ARG0'], ['effort-01', 'and', 'ARG1'], ['and', 'logistics', 'op1'], ['and', 'distribute-01', 'op2'], ['increase-01', 'surge-01', 'time'], ['surge-01', 'demand-01', 'ARG1'], ['surge-01', 'crisis', 'prep-amid'], ['crisis', 'coronavirus', 'mod'], ['name', 'Aldi', 'op1'], ['multiple', '1000', 'op1'], ['name', 'UK', 'op1']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dashi/Desktop/SR4NLP - Semantic Representations for NLP/final project/nlp_env/lib/python3.7/site-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('test_AMR.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    next(csv_reader, None)\n",
    "    g_test = []\n",
    "    for row in tqdm(csv_reader, total=400):\n",
    "        AP = AMRPlot()\n",
    "        AP.build_from_graph(entry = row)\n",
    "        edges = get_edges(AP.graph, label=True)\n",
    "        g_test.append(edges)\n",
    "\n",
    "    np.save('g_test',g_test)\n",
    "    print(g_test[1])\n",
    "\n",
    "len(g_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600,), (400,), (2000,), numpy.ndarray)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gtrs = np.load('g_train.npy',allow_pickle=True)\n",
    "gtes = np.load('g_test.npy',allow_pickle=True)\n",
    "gall = np.concatenate((gtrs, gtes), axis=0)\n",
    "gtrs.shape, gtes.shape, gall.shape, type(gtrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate inventories for words and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5138, 109)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_set = list({ts[i] for g in gall for ts in g for i in range(2)})\n",
    "edge_set = list({ts[2] for g in gall for ts in g})\n",
    "word_set.sort()\n",
    "edge_set.sort()\n",
    "word_to_id = dict(zip(word_set,[i for i in range(len(word_set))]))\n",
    "edge_to_id = dict(zip(edge_set,[i for i in range(len(edge_set))]))\n",
    "Vsize, Esize = len(word_to_id), len(edge_to_id)\n",
    "Vsize, Esize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_embedding(edges):\n",
    "    # for a single tweet amr    \n",
    "    # print(edges,\"\\n\")\n",
    "    nodes = list({edge[i] for edge in edges for i in range(2)})\n",
    "    nodes_to_id = dict(zip(nodes,[i for i in range(len(nodes))]))\n",
    "    # print(nodes_to_id,\"\\n\")\n",
    "    edge_index = [[nodes_to_id[edge[0]] for edge in edges], [nodes_to_id[edge[1]] for edge in edges]]\n",
    "    x, edge_attr = [], []\n",
    "    for node in nodes_to_id.keys():\n",
    "        vector = np.zeros(Vsize)\n",
    "        # one-hot vector\n",
    "        vector[word_to_id[node]] = 1.0\n",
    "        x.append(vector)\n",
    "\n",
    "    for edge in edges:\n",
    "        vector = np.zeros(Esize)\n",
    "        # one-hot vector\n",
    "        vector[edge_to_id[edge[2]]] = 1.0\n",
    "        edge_attr.append(vector)\n",
    "\n",
    "    return np.array(x), np.array(edge_index), np.array(edge_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get AMR classes (labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 400, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def label_converter(labels):\n",
    "    nlabels = []\n",
    "    for label in labels:\n",
    "        if label == 'Positive':\n",
    "            nlabels.append(2)\n",
    "        elif label == 'Negative':\n",
    "            nlabels.append(0)\n",
    "        else:\n",
    "            nlabels.append(1)\n",
    "    return nlabels\n",
    "\n",
    "with open('train_label.csv', 'r') as train_label, open('test_label.csv', 'r') as test_label:\n",
    "    y_train = train_label.read().split('\\n')\n",
    "    y_test = test_label.read().split('\\n')\n",
    "    y_train = label_converter(y_train)\n",
    "    y_test = label_converter(y_test)\n",
    "\n",
    "len(y_train), len(y_test), y_train[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate datasets for GNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 400)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def get_dataset(graph,labels):\n",
    "    dataset = []\n",
    "    for i in range(len(graph)):\n",
    "        x, edge_index, edge_attr = data_embedding(gtrs[i])\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "        dataset.append(Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=[labels[i]]))\n",
    "    return dataset\n",
    "\n",
    "train_dataset = get_dataset(gtrs,y_train)\n",
    "test_dataset = get_dataset(gtes, y_test)\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2fd3eaa0214f9ca1646bc965ffe9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2]) [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(Vsize, 16)\n",
    "        self.conv2 = GCNConv(16, 3)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "# print(data.x.shape)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "model.train()\n",
    "for epoch in tqdm(range(10), total=10):\n",
    "    for i in range(1600):\n",
    "        # print(i)\n",
    "        if not gtrs[i]:\n",
    "            continue\n",
    "        data = train_dataset[i]\n",
    "        optimizer.zero_grad()\n",
    "        # print(data)\n",
    "        out = model(data)\n",
    "        # print(data)\n",
    "        # print(out.shape, torch.reshape(torch.tensor(data.y),out.shape))\n",
    "        loss = F.nll_loss(out, torch.tensor(data.y*np.ones(out.shape[0]), dtype=torch.long))\n",
    "        loss.backward()\n",
    "        # print(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "# correct = (pred == data.y*np.ones(pred.shape[0])).sum()\n",
    "# acc = int(correct) / int()\n",
    "# print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 0, 2, 0, 2, 0, 0]) [0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "data = test_dataset[8]\n",
    "model.eval()\n",
    "pred = model(data).argmax(dim=1)\n",
    "print(pred, data.y*np.ones(pred.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e76047571534d9678569ee9117847be362aff1ac80d7491f5f35fa09585552a9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('nlp_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
