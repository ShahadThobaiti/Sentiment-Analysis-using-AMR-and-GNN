{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Iterable\n",
    "from pydot import Dot, graph_from_dot_data, Edge\n",
    "from graphviz.graphs import BaseGraph\n",
    "from graphviz import Source\n",
    "import amrlib\n",
    "from amrlib.graph_processing.amr_plot import AMRPlot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv, pickle\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract nodes and edges from AMR graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_dot_obj(graph_spec) -> List[Dot]:\n",
    "    \"\"\"Get a dot (graphs) object list from a variety \n",
    "    of possible sources (postelizing inputs here)\"\"\"\n",
    "    _original_graph_spec = graph_spec\n",
    "    if isinstance(graph_spec, (BaseGraph, Source)):\n",
    "        # get the source (str) from a graph object\n",
    "        graph_spec = graph_spec.source\n",
    "    if isinstance(graph_spec, str):\n",
    "        # get a dot-graph from dot string data\n",
    "        graph_spec = graph_from_dot_data(graph_spec)\n",
    "    # make sure we have a list of Dot objects now\n",
    "    assert isinstance(graph_spec, list) and all(\n",
    "        isinstance(x, Dot) for x in graph_spec\n",
    "    ), (\n",
    "        f\"Couldn't get a proper dot object list from: {_original_graph_spec}. \"\n",
    "        f\"At this point, we should have a list of Dot objects, but was: {graph_spec}\"\n",
    "    )\n",
    "    return graph_spec\n",
    "\n",
    "def get_edges(graph_spec, label = False):\n",
    "    \"\"\"Get a list of edges for a given graph (or list of lists thereof).\n",
    "    If ``postprocess_edges`` is ``None`` the function will return ``pydot.Edge`` objects from\n",
    "    which you can extract any information you want.\n",
    "    By default though, it is set to extract the node pairs for the edges, and you can\n",
    "    replace with any function that takes ``pydot.Edge`` as an input.\n",
    "    \"\"\"\n",
    "    graphs = get_graph_dot_obj(graph_spec)\n",
    "    n_graphs = len(graphs)\n",
    "    if n_graphs > 1:\n",
    "        return [get_edges(graph) for graph in graphs]\n",
    "    elif n_graphs == 0:\n",
    "        raise ValueError(f\"Your input had no graphs\")\n",
    "    else:\n",
    "        graph = graphs[0]\n",
    "        edges = graph.get_edges()\n",
    "        edges_list = []\n",
    "        if not label:\n",
    "            for edge in edges:\n",
    "                r1, r2 = graph.get_node(edge.get_source())[0].get_label().strip('\\\"').strip('\\\\').strip('\\\"'), graph.get_node(edge.get_destination())[0].get_label().strip('\\\"').strip('\\\\').strip('\\\"')\n",
    "                if '/' in r1:\n",
    "                    r1 = r1.split('/')[1]\n",
    "                elif '\\\\' in r1:\n",
    "                    r1 = r1.split('\\\\')[0]\n",
    "                \n",
    "                if '/' in r2:\n",
    "                    r2 = r2.split('/')[1]\n",
    "                elif '\\\\' in r1:\n",
    "                    r2 = r2.split('\\\\')[0]\n",
    "\n",
    "                edges_list.append([r1,r2])\n",
    "        else:\n",
    "            for edge in edges:\n",
    "                r1, r2, r3 = graph.get_node(edge.get_source())[0].get_label().strip('\\\"').strip('\\\\').strip('\\\"'), graph.get_node(edge.get_destination())[0].get_label().strip('\\\"').strip('\\\\').strip('\\\"'), edge.get_label().strip('\\\"')[1:]\n",
    "                if '/' in r1:\n",
    "                    r1 = r1.split('/')[1]\n",
    "                elif '\\\\' in r1:\n",
    "                    r1 = r1.split('\\\\')[0]\n",
    "                \n",
    "                if '/' in r2:\n",
    "                    r2 = r2.split('/')[1]\n",
    "                elif '\\\\' in r1:\n",
    "                    print(\"called\")\n",
    "                    r2 = r2.split('\\\\')[0]\n",
    "\n",
    "                edges_list.append([r1,r2,r3])\n",
    "        \n",
    "        return edges_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save large intermediate results (Only used for the first run). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_AMR.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    next(csv_reader, None)\n",
    "    g_train = []\n",
    "    for row in tqdm(csv_reader, total=1600):\n",
    "        AP = AMRPlot()\n",
    "        AP.build_from_graph(entry = row)\n",
    "        edges = get_edges(AP.graph, label=True)\n",
    "        g_train.append(edges)\n",
    "\n",
    "    np.save('g_train',g_train)\n",
    "\n",
    "with open('test_AMR.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    next(csv_reader, None)\n",
    "    g_test = []\n",
    "    for row in tqdm(csv_reader, total=400):\n",
    "        AP = AMRPlot()\n",
    "        AP.build_from_graph(entry = row)\n",
    "        edges = get_edges(AP.graph, label=True)\n",
    "        g_test.append(edges)\n",
    "\n",
    "    np.save('g_test',g_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600,) (400,) (2000,) <class 'numpy.ndarray'>\n",
      "[['find-01', 'i', 'ARG0'], ['find-01', 'this', 'ARG1'], ['find-01', 'company', 'location'], ['company', 'name', 'name'], ['name', 'Target', 'op1']]\n"
     ]
    }
   ],
   "source": [
    "gtrs = np.load('g_train.npy',allow_pickle=True) # list of edges for each \n",
    "gtes = np.load('g_test.npy',allow_pickle=True)\n",
    "gall = np.concatenate((gtrs, gtes), axis=0)\n",
    "print(gtrs.shape, gtes.shape, gall.shape, type(gtrs))\n",
    "print(gtrs[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate inventories for words and edge labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5138, 109)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_set = list({ts[i] for g in gall for ts in g for i in range(2)})\n",
    "edge_set = list({ts[2] for g in gall for ts in g})\n",
    "word_set.sort()\n",
    "edge_set.sort()\n",
    "word_to_id = dict(zip(word_set,[i for i in range(len(word_set))]))\n",
    "edge_to_id = dict(zip(edge_set,[i for i in range(len(edge_set))]))\n",
    "Vsize, Esize = len(word_to_id), len(edge_to_id)\n",
    "Vsize, Esize # number of node features and number of edge lables in our corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Embedding. Get x, edge_index, edge_attr for each graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_embedding(edges):\n",
    "    # for a single tweet amr    \n",
    "    # print(edges,\"\\n\")\n",
    "    nodes = list({edge[i] for edge in edges for i in range(2)})\n",
    "    nodes_to_id = dict(zip(nodes,[i for i in range(len(nodes))]))\n",
    "    edge_index = [[nodes_to_id[edge[0]] for edge in edges], [nodes_to_id[edge[1]] for edge in edges]]\n",
    "    x, edge_attr = [], []\n",
    "    for node in nodes_to_id.keys():\n",
    "        vector = np.zeros(Vsize)\n",
    "        vector[word_to_id[node]] = 1.0 # one-hot vector\n",
    "        x.append(vector)\n",
    "\n",
    "    for edge in edges:\n",
    "        vector = np.zeros(Esize)\n",
    "        vector[edge_to_id[edge[2]]] = 1.0 # one-hot vector\n",
    "        edge_attr.append(vector)\n",
    "\n",
    "    return np.array(x), np.array(edge_index), np.array(edge_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get AMR classes (labels) and represent them as numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 400\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def label_converter(labels):\n",
    "    nlabels = []\n",
    "    for label in labels:\n",
    "        if label == 'Positive':\n",
    "            nlabels.append(2)\n",
    "        elif label == 'Negative':\n",
    "            nlabels.append(0)\n",
    "        else:\n",
    "            nlabels.append(1)\n",
    "    return nlabels\n",
    "\n",
    "with open('train_label.csv', 'r') as train_label, open('test_label.csv', 'r') as test_label:\n",
    "    y_train = train_label.read().split('\\n')\n",
    "    y_test = test_label.read().split('\\n')\n",
    "    y_train = label_converter(y_train)\n",
    "    y_test = label_converter(y_test)\n",
    "\n",
    "print(len(y_train), len(y_test))\n",
    "print(y_train[4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate datasets for GNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 400\n",
      "Data(x=[6, 5138], edge_index=[2, 5], edge_attr=[5, 109], y=[1]) Data(x=[24, 5138], edge_index=[2, 23], edge_attr=[23, 109], y=[1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def get_dataset(graph,labels):\n",
    "    dataset = []\n",
    "    for i in range(len(graph)):\n",
    "        x, edge_index, edge_attr = data_embedding(graph[i])\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "        dataset.append(Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=[labels[i]]))\n",
    "    return dataset\n",
    "\n",
    "train_dataset = get_dataset(gtrs,y_train)\n",
    "test_dataset = get_dataset(gtes, y_test)\n",
    "print(len(train_dataset), len(test_dataset))\n",
    "print(train_dataset[4], test_dataset[48])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a598d36476463d9e213744962cfe70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0857911736517667\n",
      "1.0810372656465417\n",
      "1.0784098096645314\n",
      "1.074333666934179\n",
      "1.0753057458361015\n",
      "1.0733996142873696\n",
      "1.0702615687474466\n",
      "1.0716281396018856\n",
      "1.0696270608562144\n",
      "1.0667744054841468\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(Vsize, 16)\n",
    "        self.conv2 = GCNConv(16, 3)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "model.train()\n",
    "for epoch in tqdm(range(10), total=10):\n",
    "    avg_loss = []\n",
    "    for i in range(1600):\n",
    "        if not gtrs[i]: # ignore empty graph\n",
    "            continue\n",
    "        data = train_dataset[i]\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, torch.tensor(data.y*np.ones(out.shape[0]), dtype=torch.long))\n",
    "        loss.backward()\n",
    "        avg_loss.append(loss.item())\n",
    "        optimizer.step()\n",
    "    print(np.average(avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Precision: 0.3907995774193282\n",
      "Graph Precision: 0.3911917098445596\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "precision = []\n",
    "accuracy = []\n",
    "for i in range(400):\n",
    "    if not gtes[i]: \n",
    "        continue # ignore empty graph\n",
    "    pred = model(test_dataset[i]).argmax(dim=1)\n",
    "    correct_node = (pred.numpy() == test_dataset[i].y*np.ones(pred.shape[0])).astype(np.float32).sum()\n",
    "    # print(correct_node, test_dataset[i].y)\n",
    "    precision.append(correct_node/pred.shape[0])\n",
    "    accuracy.append(1 if correct_node >= 0.4*pred.shape[0] else 0)\n",
    "\n",
    "print(\"Node Precision:\", np.average(precision))\n",
    "print(\"Graph Precision:\",np.array(accuracy).sum()/np.array(accuracy).shape[0])\n",
    "\n",
    "# for i in range(10,90):\n",
    "#     if not gtes[i]: \n",
    "#         continue # ignore empty graph\n",
    "# print(gtes[2])\n",
    "# print(test_dataset[2])\n",
    "# print(model(test_dataset[2]).argmax(dim=1), test_dataset[3].y)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e76047571534d9678569ee9117847be362aff1ac80d7491f5f35fa09585552a9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('nlp_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
