{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Iterable\n",
    "from pydot import Dot, graph_from_dot_data, Edge\n",
    "from graphviz.graphs import BaseGraph\n",
    "from graphviz import Source\n",
    "import amrlib\n",
    "from amrlib.graph_processing.amr_plot import AMRPlot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv, pickle\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract nodes and edges from AMR graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cite: https://stackoverflow.com/questions/47426249/finding-list-of-edges-in-graphviz-in-python \n",
    "def get_graph_dot_obj(graph_spec) -> List[Dot]:\n",
    "    \"\"\"Get a dot (graphs) object list from a variety \n",
    "    of possible sources (postelizing inputs here)\"\"\"\n",
    "    _original_graph_spec = graph_spec\n",
    "    if isinstance(graph_spec, (BaseGraph, Source)):\n",
    "        # get the source (str) from a graph object\n",
    "        graph_spec = graph_spec.source\n",
    "    if isinstance(graph_spec, str):\n",
    "        # get a dot-graph from dot string data\n",
    "        graph_spec = graph_from_dot_data(graph_spec)\n",
    "    # make sure we have a list of Dot objects now\n",
    "    assert isinstance(graph_spec, list) and all(\n",
    "        isinstance(x, Dot) for x in graph_spec\n",
    "    ), (\n",
    "        f\"Couldn't get a proper dot object list from: {_original_graph_spec}. \"\n",
    "        f\"At this point, we should have a list of Dot objects, but was: {graph_spec}\"\n",
    "    )\n",
    "    return graph_spec\n",
    "\n",
    "def get_edges(graph_spec, label = False):\n",
    "    \"\"\"Get a list of edges for a given graph (or list of lists thereof).\n",
    "    If ``postprocess_edges`` is ``None`` the function will return ``pydot.Edge`` objects from\n",
    "    which you can extract any information you want.\n",
    "    By default though, it is set to extract the node pairs for the edges, and you can\n",
    "    replace with any function that takes ``pydot.Edge`` as an input.\n",
    "    \"\"\"\n",
    "    graphs = get_graph_dot_obj(graph_spec)\n",
    "    n_graphs = len(graphs)\n",
    "    if n_graphs > 1:\n",
    "        return [get_edges(graph) for graph in graphs]\n",
    "    elif n_graphs == 0:\n",
    "        raise ValueError(f\"Your input had no graphs\")\n",
    "    else:\n",
    "        graph = graphs[0]\n",
    "        edges = graph.get_edges()\n",
    "        edges_list = []\n",
    "        if not label:\n",
    "            for edge in edges:\n",
    "                r1, r2 = graph.get_node(edge.get_source())[0].get_label().strip('\\\"').strip('\\\\').strip('\\\"'), graph.get_node(edge.get_destination())[0].get_label().strip('\\\"').strip('\\\\').strip('\\\"')\n",
    "                if '/' in r1:\n",
    "                    r1 = r1.split('/')[1]\n",
    "                elif '\\\\' in r1:\n",
    "                    r1 = r1.split('\\\\')[0]\n",
    "                \n",
    "                if '/' in r2:\n",
    "                    r2 = r2.split('/')[1]\n",
    "                elif '\\\\' in r1:\n",
    "                    r2 = r2.split('\\\\')[0]\n",
    "\n",
    "                edges_list.append([r1,r2])\n",
    "        else:\n",
    "            for edge in edges:\n",
    "                r1, r2, r3 = graph.get_node(edge.get_source())[0].get_label().strip('\\\"').strip('\\\\').strip('\\\"'), graph.get_node(edge.get_destination())[0].get_label().strip('\\\"').strip('\\\\').strip('\\\"'), edge.get_label().strip('\\\"')[1:]\n",
    "                if '/' in r1:\n",
    "                    r1 = r1.split('/')[1]\n",
    "                elif '\\\\' in r1:\n",
    "                    r1 = r1.split('\\\\')[0]\n",
    "                \n",
    "                if '/' in r2:\n",
    "                    r2 = r2.split('/')[1]\n",
    "                elif '\\\\' in r1:\n",
    "                    print(\"called\")\n",
    "                    r2 = r2.split('\\\\')[0]\n",
    "\n",
    "                edges_list.append([r1,r2,r3])\n",
    "        \n",
    "        return edges_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save large intermediate results (Only used for the first run). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420c7bfd87f94cf18d2c79ab876c5fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring epigraph data for duplicate triple: ('c2', ':ARG0', 'p')\n",
      "ignoring epigraph data for duplicate triple: ('h', ':ARG0', 'y')\n",
      "ignoring epigraph data for duplicate triple: ('c3', ':ARG1', 'w')\n",
      "ignoring epigraph data for duplicate triple: ('c', ':name', 'n')\n",
      "ignoring epigraph data for duplicate triple: ('c', ':ARG0', 'p2')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['multi-sentence', 'acquire-01', 'snt1'], ['acquire-01', 'country', 'ARG0'], ['country', 'name', 'name'], ['acquire-01', 'stake', 'ARG1'], ['stake', 'company', 'mod'], ['company', 'name', 'name'], ['stake', 'percentage-entity', 'quant'], ['stake', 'mean-01', 'ARG1-of'], ['mean-01', 'approximately', 'ARG2'], ['approximately', 'share-01', 'op1'], ['multi-sentence', 'and', 'snt2'], ['and', 'price-01', 'op1'], ['price-01', 'share-01', 'ARG1'], ['price-01', 'monetary-quantity', 'ARG2'], ['monetary-quantity', 'low-04', 'ARG1-of'], ['low-04', 'very', 'degree'], ['and', 'lose-02', 'op2'], ['lose-02', 'stock', 'ARG0'], ['lose-02', 'percentage-entity', 'ARG1'], ['percentage-entity', 'include-91', 'ARG3-of'], ['include-91', 'value', 'ARG2'], ['lose-02', 'since', 'time'], ['since', 'begin-01', 'op1'], ['begin-01', 'year', 'ARG1'], ['lose-02', 'since', 'time'], ['since', 'struggle-02', 'op1'], ['struggle-02', 'industry', 'ARG0'], ['industry', 'cruise-01', 'mod'], ['struggle-02', 'company', 'ARG1'], ['company', 'name', 'name'], ['name', 'Saudi', 'op1'], ['name', 'Arabia', 'op2'], ['name', 'Carnival', 'op1'], ['percentage-entity', '8.2', 'value'], ['share-01', '43500000', 'quant'], ['percentage-entity', '81', 'value'], ['name', 'Coronav', 'op1']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dashi/Desktop/SR4NLP - Semantic Representations for NLP/final project/nlp_env/lib/python3.7/site-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1600, 0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('train_AMR.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    next(csv_reader, None)\n",
    "    g_train = []\n",
    "    for row in tqdm(csv_reader, total=1600):\n",
    "        AP = AMRPlot()\n",
    "        AP.build_from_graph(entry = row)\n",
    "        edges = get_edges(AP.graph, label=True)\n",
    "        g_train.append(edges)\n",
    "\n",
    "    np.save('g_train',g_train)\n",
    "    print(g_train[1])\n",
    "\n",
    "len(g_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66c37180a0b49189f800f01e1fef0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring epigraph data for duplicate triple: ('b', ':mod', 'p3')\n",
      "ignoring epigraph data for duplicate triple: ('c', ':ARG0', 'h')\n",
      "ignoring epigraph data for duplicate triple: ('w2', ':ARG0', 'p')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['create-01', 'company', 'ARG0'], ['company', 'name', 'name'], ['create-01', 'job', 'ARG1'], ['job', 'new-01', 'ARG1-of'], ['job', 'multiple', 'quant'], ['create-01', 'footprint', 'location'], ['footprint', 'company', 'poss'], ['footprint', 'country', 'location'], ['country', 'name', 'name'], ['create-01', 'increase-01', 'purpose'], ['increase-01', 'company', 'ARG0'], ['increase-01', 'effort-01', 'ARG1'], ['effort-01', 'company', 'ARG0'], ['effort-01', 'and', 'ARG1'], ['and', 'logistics', 'op1'], ['and', 'distribute-01', 'op2'], ['increase-01', 'surge-01', 'time'], ['surge-01', 'demand-01', 'ARG1'], ['surge-01', 'crisis', 'prep-amid'], ['crisis', 'coronavirus', 'mod'], ['name', 'Aldi', 'op1'], ['multiple', '1000', 'op1'], ['name', 'UK', 'op1']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dashi/Desktop/SR4NLP - Semantic Representations for NLP/final project/nlp_env/lib/python3.7/site-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('test_AMR.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    next(csv_reader, None)\n",
    "    g_test = []\n",
    "    for row in tqdm(csv_reader, total=400):\n",
    "        AP = AMRPlot()\n",
    "        AP.build_from_graph(entry = row)\n",
    "        edges = get_edges(AP.graph, label=True)\n",
    "        g_test.append(edges)\n",
    "\n",
    "    np.save('g_test',g_test)\n",
    "    print(g_test[1])\n",
    "\n",
    "len(g_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600,), (400,), (2000,), numpy.ndarray)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gtrs = np.load('g_train.npy',allow_pickle=True)\n",
    "gtes = np.load('g_test.npy',allow_pickle=True)\n",
    "gall = np.concatenate((gtrs, gtes), axis=0)\n",
    "gtrs.shape, gtes.shape, gall.shape, type(gtrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate inventories for words and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5138, 109)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_set = list({ts[i] for g in gall for ts in g for i in range(2)})\n",
    "edge_set = list({ts[2] for g in gall for ts in g})\n",
    "word_set.sort()\n",
    "edge_set.sort()\n",
    "word_to_id = dict(zip(word_set,[i for i in range(len(word_set))]))\n",
    "edge_to_id = dict(zip(edge_set,[i for i in range(len(edge_set))]))\n",
    "Vsize, Esize = len(word_to_id), len(edge_to_id)\n",
    "Vsize, Esize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['possible-01', 'work-01', 'ARG1'], ['work-01', 'you', 'ARG0'], ['work-01', 'hard-02', 'ARG1-of'], ['hard-02', 'have-degree-91', 'ARG2-of'], ['have-degree-91', 'work-01', 'ARG1'], ['have-degree-91', 'too', 'ARG3'], ['possible-01', 'find-01', 'condition'], ['find-01', 'you', 'ARG0'], ['find-01', 'dream-01', 'ARG1'], ['dream-01', 'you', 'ARG0'], ['dream-01', 'and', 'ARG1'], ['and', 'product', 'op1'], ['product', 'clean-01', 'purpose'], ['and', 'index', 'op2'], ['index', 'market', 'mod'], ['market', 'stock', 'mod']] \n",
      "\n",
      "{'find-01': 0, 'you': 1, 'possible-01': 2, 'have-degree-91': 3, 'product': 4, 'work-01': 5, 'clean-01': 6, 'index': 7, 'and': 8, 'market': 9, 'stock': 10, 'dream-01': 11, 'too': 12, 'hard-02': 13} \n",
      "\n",
      "[[2, 5, 5, 13, 3, 3, 2, 0, 0, 11, 11, 8, 4, 8, 7, 9], [5, 1, 13, 3, 5, 12, 0, 1, 11, 1, 8, 4, 6, 7, 9, 10]] \n",
      "\n",
      "(2, 16)\n",
      "(14, 5138)\n",
      "(16, 109)\n"
     ]
    }
   ],
   "source": [
    "# for a single tweet amr\n",
    "edges = gtrs[0]\n",
    "print(edges,\"\\n\")\n",
    "nodes = list({edge[i] for edge in edges for i in range(2)})\n",
    "nodes_to_id = dict(zip(nodes,[i for i in range(len(nodes))]))\n",
    "print(nodes_to_id,\"\\n\")\n",
    "edge_index = [[nodes_to_id[edge[0]] for edge in edges], [nodes_to_id[edge[1]] for edge in edges]]\n",
    "x, edge_attr = [], []\n",
    "for node in nodes_to_id.keys():\n",
    "    vector = np.zeros(Vsize)\n",
    "    # one-hot vector\n",
    "    vector[word_to_id[node]] = 1.0\n",
    "    x.append(vector)\n",
    "\n",
    "for edge in edges:\n",
    "    vector = np.zeros(Esize)\n",
    "    # one-hot vector\n",
    "    vector[edge_to_id[edge[2]]] = 1.0\n",
    "    edge_attr.append(vector)\n",
    "\n",
    "print(edge_index, \"\\n\")\n",
    "print(np.array(edge_index).shape)\n",
    "print(np.array(x).shape)\n",
    "print(np.array(edge_attr).shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e76047571534d9678569ee9117847be362aff1ac80d7491f5f35fa09585552a9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('nlp_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
